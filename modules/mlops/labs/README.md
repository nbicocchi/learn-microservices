# Labs

## Questions

1. What is feedback delay, and why does it complicate real-time model performance evaluation?
2. What is data drift, and how can it impact the performance of a deployed ML model?
3. How can concept drift and prediction drift impact the performance of a machine learning model in production, and what strategies can be used to detect them?
4. Why is maintaining data quality crucial for ML models, and how can poor data quality lead to misleading model predictions over time?
5. What are the different MLOps maturity levels, and how do they reflect an organization's ability to automate and manage the ML lifecycle effectively?
6. Give an example of a rule-based check that could be useful for monitoring model predictions in production.
7. How does the concept of Pipeline-As-Code support Continuous Integration and Continuous Deployment (CI/CD) in machine learning workflows?
8. List and briefly describe the key stages in the machine learning development lifecycle.
9. What is the purpose of a feature store in an MLOps system, and how does it differ from a data warehouse?
10. Explain the role of experiment tracking and name two popular tools used for it.



